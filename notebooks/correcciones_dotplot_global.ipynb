{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be2386-fb8b-4eb6-8f30-f69d6b299895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# correcciones_dotplot_global.ipynb (VERSIÓN LIMPIA post-RBC-out + Level2_final)\n",
    "# Objetivo:\n",
    "#   - Generar DOTPLOT GLOBAL FINAL a Level2 (FINAL) desde OUT_FILTER (ya sin RBC)\n",
    "#   - Memory-safe: abre backed=\"r\" y solo carga ~50 genes a RAM\n",
    "#   - Aplica Level2_final_map.json (Conv_T_other -> CD4_Memory) para plot/orden/tablas\n",
    "#   - Guarda: figura PNG + tablas (totales + marcadores usados)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Paths / proyecto\n",
    "# ------------------------------\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data_processed\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No encuentro 'data_processed' subiendo desde: {start}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\"\n",
    "\n",
    "OUT_FILTER = DATA_PROCESSED / \"TFM_CIRRHOSIS_main_filtered_for_analysis.h5ad\"\n",
    "\n",
    "FIG_DIR = PROJECT_ROOT / \"figures_final\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_SUMMARY = PROJECT_ROOT / \"summary_tables_final\"\n",
    "OUT_SUMMARY.mkdir(exist_ok=True)\n",
    "\n",
    "MAP_PATH = OUT_SUMMARY / \"Level2_final_map.json\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"OUT_FILTER  :\", OUT_FILTER)\n",
    "print(\"FIG_DIR     :\", FIG_DIR)\n",
    "print(\"OUT_SUMMARY :\", OUT_SUMMARY)\n",
    "print(\"MAP_PATH    :\", MAP_PATH)\n",
    "\n",
    "if not OUT_FILTER.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No existe OUT_FILTER:\\n{OUT_FILTER}\\n¿Has ejecutado NBXX y guardado el objeto final?\"\n",
    "    )\n",
    "\n",
    "if not MAP_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No existe Level2_final_map.json:\\n{MAP_PATH}\\n\"\n",
    "        \"Este notebook requiere ese mapa (p.ej. Conv_T_other -> CD4_Memory).\"\n",
    "    )\n",
    "\n",
    "with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    level2_map = json.load(f)\n",
    "\n",
    "print(\"\\nLevel2_final_map.json loaded:\")\n",
    "print(level2_map)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Abrir en backed + checks rápidos\n",
    "# ------------------------------\n",
    "adata_b = sc.read_h5ad(OUT_FILTER, backed=\"r\")\n",
    "print(\"\\nLoaded OUT_FILTER (backed):\", adata_b)\n",
    "\n",
    "# checks mínimos\n",
    "for col in [\"Level2\", \"Level1\", \"Level1_refined\"]:\n",
    "    if col not in adata_b.obs.columns:\n",
    "        adata_b.file.close()\n",
    "        raise KeyError(f\"Falta columna requerida en obs: '{col}'\")\n",
    "\n",
    "if \"log1p_10k\" not in adata_b.layers.keys():\n",
    "    adata_b.file.close()\n",
    "    raise KeyError(f\"No existe layer 'log1p_10k' en el objeto. layers={list(adata_b.layers.keys())}\")\n",
    "\n",
    "if \"doublet_like\" in adata_b.obs.columns:\n",
    "    print(\"doublet_like True (debería ser 0):\", int(adata_b.obs[\"doublet_like\"].sum()))\n",
    "\n",
    "# RBC-out sanity (debería ser 0)\n",
    "rbc_l2  = int((adata_b.obs[\"Level2\"].astype(str) == \"RBC\").sum())\n",
    "rbc_l1  = int((adata_b.obs[\"Level1\"].astype(str) == \"RBC\").sum())\n",
    "rbc_l1r = int((adata_b.obs[\"Level1_refined\"].astype(str) == \"RBC\").sum())\n",
    "print(\"RBC counts (Level2/Level1/Level1_refined) deberían ser 0:\", (rbc_l2, rbc_l1, rbc_l1r))\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Import markers.py + overrides (SIN RBC)\n",
    "# ------------------------------\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from AI_Package.markers.markers import geneMarkers_level2, geneMarkers_level1, symbols_to_varnames\n",
    "\n",
    "# base: dict Level2 -> lista genes\n",
    "lvl2_to_symbols = {}\n",
    "for _l1, subdict in geneMarkers_level2.items():\n",
    "    for _l2, genes in subdict.items():\n",
    "        if genes:\n",
    "            lvl2_to_symbols[_l2] = list(genes)\n",
    "\n",
    "# overrides mínimos para poblaciones problemáticas / nuevas por Level2_final_map\n",
    "# (SIN RBC)\n",
    "OVERRIDE_2MARKERS = {\n",
    "    \"B_Other\":      [\"MS4A1\", \"CD74\"],\n",
    "    \"CD4_Memory\":   [\"IL7R\", \"CCR7\"],     # <- clave tras Conv_T_other -> CD4_Memory\n",
    "    \"ISG_Myeloid\":  [\"ISG15\", \"IFIT3\"],\n",
    "    \"MonoDC_Other\": [\"LYZ\", \"FCER1G\"],\n",
    "    \"DC3\":          [\"CD1C\", \"S100A8\"],   # <- asegurar que DC3 tenga 2 marcadores (aparece en tu dataset)\n",
    "    \"DC4\":          [\"FCGR3A\", \"LST1\"],   # <- fallback si markers.py no lo cubre bien\n",
    "    \"HSCs\":         [\"CD34\", \"KIT\"],\n",
    "    \"Plasma\":       [\"MZB1\", \"JCHAIN\"],\n",
    "    \"pDC\":          [\"IL3RA\", \"IRF7\"],\n",
    "}\n",
    "for l2, genes2 in OVERRIDE_2MARKERS.items():\n",
    "    if (l2 not in lvl2_to_symbols) or (len([g for g in lvl2_to_symbols.get(l2, []) if g]) < 2):\n",
    "        lvl2_to_symbols[l2] = genes2\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Preparar obs + Level2_final\n",
    "# ------------------------------\n",
    "obs = adata_b.obs.copy()\n",
    "\n",
    "obs[\"Level2_final\"] = obs[\"Level2\"].astype(str).replace(level2_map).astype(str)\n",
    "obs[\"Level2_final\"] = pd.Categorical(obs[\"Level2_final\"])\n",
    "\n",
    "print(\"\\n[CHECK] Conv_T_other remaining en Level2_final (debe ser 0):\",\n",
    "      int((obs[\"Level2_final\"].astype(str) == \"Conv_T_other\").sum()))\n",
    "print(\"[CHECK] CD4_Memory count en Level2_final:\",\n",
    "      int((obs[\"Level2_final\"].astype(str) == \"CD4_Memory\").sum()))\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Orden por bloques (SIN RBC) usando Level2_final\n",
    "#   - Ajuste clave: DC3 se considera DC para orden/etiqueta\n",
    "# ------------------------------\n",
    "order_by_group = {\n",
    "    \"B\":     [\"B_Naive\", \"B_Memory\", \"B_Activated\", \"B_Atypical\", \"B_Other\"],\n",
    "    \"Plasma\":[\"Plasma\"],\n",
    "    \"pDC\":   [\"pDC\"],\n",
    "    \"T\":     [\"CD4_Naive\",\"CD4_Memory\",\"CD8_Naive\",\"CD8_Effector_Cytotoxic\",\"Treg\",\"MAIT\",\"GammaDelta_T\",\"Proliferative_T\",\"Exhausted_T\"],\n",
    "    \"NK\":    [\"NK\"],\n",
    "    \"Mono\":  [\"Classical_Mono\",\"NonClassical_Mono\",\"ISG_Myeloid\",\"MonoDC_Other\"],\n",
    "    \"DC\":    [\"cDC1\",\"cDC2\",\"DC3\",\"DC4\",\"aDC\"],  # <- DC3 añadido\n",
    "    \"HSCs\":  [\"HSCs\"],\n",
    "}\n",
    "\n",
    "present_l2 = sorted(set(obs[\"Level2_final\"].astype(str).dropna().unique()))\n",
    "\n",
    "# construir orden respetando bloques\n",
    "level2_order = []\n",
    "for g, l2_list in order_by_group.items():\n",
    "    for l2 in l2_list:\n",
    "        if l2 in present_l2:\n",
    "            level2_order.append(l2)\n",
    "\n",
    "extras = [x for x in present_l2 if x not in level2_order]\n",
    "level2_order = level2_order + sorted(extras)\n",
    "\n",
    "def group_of_l2(l2: str) -> str:\n",
    "    for g, l2_list in order_by_group.items():\n",
    "        if l2 in l2_list:\n",
    "            return g\n",
    "    return \"Other\"\n",
    "\n",
    "# Label final para plot: \"Grupo | Level2_final\"\n",
    "obs[\"Level2_plot\"] = obs[\"Level2_final\"].astype(str).map(lambda l2: f\"{group_of_l2(l2)} | {l2}\")\n",
    "level2_plot_order = [f\"{group_of_l2(l2)} | {l2}\" for l2 in level2_order]\n",
    "obs[\"Level2_plot\"] = pd.Categorical(obs[\"Level2_plot\"], categories=level2_plot_order, ordered=True)\n",
    "\n",
    "print(\"\\n[CHECK] Level2_final presentes:\", len(present_l2))\n",
    "print(\"[CHECK] Primeros 30:\", present_l2[:30])\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Lista final de genes (2 por Level2_final) + mapeo symbols->varnames\n",
    "# ------------------------------\n",
    "gene_symbols = []\n",
    "for l2 in level2_order:\n",
    "    gene_symbols.extend(lvl2_to_symbols.get(l2, [])[:2])\n",
    "\n",
    "# dedup manteniendo orden\n",
    "seen = set()\n",
    "gene_symbols = [g for g in gene_symbols if not (g in seen or seen.add(g))]\n",
    "\n",
    "gene_varnames = symbols_to_varnames(adata_b, gene_symbols)\n",
    "missing = [s for s, v in zip(gene_symbols, gene_varnames) if v is None]\n",
    "gene_varnames = [v for v in gene_varnames if v is not None]\n",
    "\n",
    "print(\"\\nMarkers symbols total:\", len(gene_symbols))\n",
    "print(\"Markers genes found :\", len(gene_varnames))\n",
    "if missing:\n",
    "    print(\"[WARN] Símbolos no encontrados (omitidos):\", missing)\n",
    "\n",
    "if len(gene_varnames) == 0:\n",
    "    adata_b.file.close()\n",
    "    raise RuntimeError(\"No se encontró ningún gen marcador en adata.var_names. Revisa symbols_to_varnames / var_names.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Cargar SOLO esos genes a RAM + dotplot FINAL\n",
    "# ------------------------------\n",
    "adata_view = adata_b[:, gene_varnames]\n",
    "adata_plot = adata_view.to_memory()   # <- aquí solo ~50 genes\n",
    "adata_b.file.close()\n",
    "\n",
    "# pasar Level2_plot al objeto pequeño\n",
    "adata_plot.obs[\"Level2_plot\"] = obs.loc[adata_plot.obs_names, \"Level2_plot\"].values\n",
    "adata_plot.obs[\"Level2_plot\"] = pd.Categorical(\n",
    "    adata_plot.obs[\"Level2_plot\"], categories=level2_plot_order, ordered=True\n",
    ")\n",
    "\n",
    "# dotplot\n",
    "sc.settings.autoshow = False\n",
    "dp = sc.pl.dotplot(\n",
    "    adata_plot,\n",
    "    var_names=gene_varnames,\n",
    "    groupby=\"Level2_plot\",\n",
    "    layer=\"log1p_10k\",\n",
    "    use_raw=False,\n",
    "    dendrogram=False,\n",
    "    standard_scale=\"var\",\n",
    "    show=False,\n",
    "    return_fig=True,\n",
    ")\n",
    "dp = dp.add_totals().style(dot_edge_color=\"black\", dot_edge_lw=0.5)\n",
    "\n",
    "out_png = FIG_DIR / \"Fig1C_Dotplot_Global_Level2_clean_FINAL.png\"  # sobrescribe si ya existía\n",
    "dp.savefig(out_png, dpi=300)\n",
    "print(\"\\nSaved:\", out_png)\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Guardar tablas auxiliares (totales + marcadores usados)\n",
    "# ------------------------------\n",
    "totals = (obs[\"Level2_plot\"].value_counts()\n",
    "          .reindex(level2_plot_order)\n",
    "          .dropna()\n",
    "          .astype(int)\n",
    "          .reset_index())\n",
    "totals.columns = [\"Level2_plot\", \"n_cells\"]\n",
    "totals[\"group\"] = totals[\"Level2_plot\"].str.split(\" \\\\| \").str[0]\n",
    "totals[\"Level2_final\"] = totals[\"Level2_plot\"].str.split(\" \\\\| \").str[1]\n",
    "\n",
    "totals_path = OUT_SUMMARY / \"QA_dotplot_totals_by_Level2_plot_noRBC.csv\"\n",
    "totals.to_csv(totals_path, index=False)\n",
    "print(\"Saved:\", totals_path)\n",
    "\n",
    "marker_rows = []\n",
    "for l2 in level2_order:\n",
    "    genes = lvl2_to_symbols.get(l2, [])\n",
    "    m1 = genes[0] if len(genes) > 0 else None\n",
    "    m2 = genes[1] if len(genes) > 1 else None\n",
    "    marker_rows.append({\"Level2_final\": l2, \"group\": group_of_l2(l2), \"marker1\": m1, \"marker2\": m2})\n",
    "\n",
    "markers_df = pd.DataFrame(marker_rows)\n",
    "markers_df[\"Level2_plot\"] = markers_df.apply(lambda r: f\"{r['group']} | {r['Level2_final']}\", axis=1)\n",
    "\n",
    "markers_path = OUT_SUMMARY / \"QA_dotplot_markers_2perLevel2_noRBC.csv\"\n",
    "markers_df.to_csv(markers_path, index=False)\n",
    "print(\"Saved:\", markers_path)\n",
    "\n",
    "print(\"\\n[OK] correcciones_dotplot_global terminado (post-RBC-out + Level2_final).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1eb796-c45f-4f1c-a007-1b029720dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRA QA — \"Dotplot en números\"\n",
    "# Genera matriz por Level2_final: mean_log1p y frac_nonzero\n",
    "# Para los marcadores usados en el dotplot (gene_varnames)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# --- paths ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data_processed\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No encuentro 'data_processed' subiendo desde: {start}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "OUT_SUMMARY = PROJECT_ROOT / \"summary_tables_final\"\n",
    "OUT_SUMMARY.mkdir(exist_ok=True)\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\"\n",
    "OUT_FILTER = DATA_PROCESSED / \"TFM_CIRRHOSIS_main_filtered_for_analysis.h5ad\"\n",
    "\n",
    "# --- cargar mapa Level2_final ---\n",
    "MAP_PATH = OUT_SUMMARY / \"Level2_final_map.json\"\n",
    "if not MAP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Falta Level2_final_map.json en: {MAP_PATH}\")\n",
    "\n",
    "with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    level2_map = json.load(f)\n",
    "\n",
    "# --- abrir backed (no cargar X entero) ---\n",
    "adata_b = sc.read_h5ad(OUT_FILTER, backed=\"r\")\n",
    "\n",
    "# sanity columns\n",
    "for col in [\"Level2\"]:\n",
    "    if col not in adata_b.obs.columns:\n",
    "        adata_b.file.close()\n",
    "        raise KeyError(f\"Falta columna obs: {col}\")\n",
    "\n",
    "# definir Level2_final en RAM (solo obs)\n",
    "obs = adata_b.obs[[\"Level2\"]].copy()\n",
    "obs[\"Level2_final\"] = obs[\"Level2\"].astype(str).replace(level2_map).astype(str)\n",
    "\n",
    "# genes usados = los del dotplot\n",
    "# REQUISITO: esta celda se ejecuta en el MISMO notebook donde existe gene_varnames\n",
    "try:\n",
    "    genes_used = list(gene_varnames)  # del notebook\n",
    "except NameError:\n",
    "    adata_b.file.close()\n",
    "    raise NameError(\"No encuentro 'gene_varnames'. Ejecuta primero las celdas del dotplot que lo definen.\")\n",
    "\n",
    "# filtrar a genes presentes (por seguridad)\n",
    "genes_used = [g for g in genes_used if g in adata_b.var_names]\n",
    "if len(genes_used) == 0:\n",
    "    adata_b.file.close()\n",
    "    raise RuntimeError(\"genes_used quedó vacío. Revisa gene_varnames vs adata.var_names.\")\n",
    "\n",
    "# --- cargar SOLO esos genes a RAM ---\n",
    "ad_small = adata_b[:, genes_used].to_memory()\n",
    "adata_b.file.close()\n",
    "\n",
    "# copiar Level2_final al objeto pequeño\n",
    "ad_small.obs[\"Level2_final\"] = obs.loc[ad_small.obs_names, \"Level2_final\"].values\n",
    "\n",
    "# --- calcular mean_log1p y frac_nonzero por grupo ---\n",
    "X = ad_small.layers[\"log1p_10k\"] if \"log1p_10k\" in ad_small.layers.keys() else ad_small.X\n",
    "if sp.issparse(X):\n",
    "    X = X.tocsr()\n",
    "\n",
    "groups = pd.Series(ad_small.obs[\"Level2_final\"].astype(str).values, index=ad_small.obs_names)\n",
    "uniq = sorted(groups.unique())\n",
    "\n",
    "rows = []\n",
    "for g in uniq:\n",
    "    idx = np.where(groups.values == g)[0]\n",
    "    n = int(idx.size)\n",
    "    if n == 0:\n",
    "        continue\n",
    "\n",
    "    Xg = X[idx, :]\n",
    "\n",
    "    # mean\n",
    "    if sp.issparse(Xg):\n",
    "        mean = np.asarray(Xg.mean(axis=0)).ravel()\n",
    "        nnz = np.asarray((Xg > 0).mean(axis=0)).ravel()\n",
    "    else:\n",
    "        mean = np.mean(np.asarray(Xg), axis=0)\n",
    "        nnz = np.mean((np.asarray(Xg) > 0), axis=0)\n",
    "\n",
    "    row = {\"Level2_final\": g, \"n_cells\": n}\n",
    "    for j, gene in enumerate(genes_used):\n",
    "        row[f\"{gene}__mean_log1p\"] = float(mean[j])\n",
    "        row[f\"{gene}__frac_nonzero\"] = float(nnz[j])\n",
    "    rows.append(row)\n",
    "\n",
    "df_num = pd.DataFrame(rows).sort_values(\"n_cells\", ascending=False)\n",
    "\n",
    "out_path = OUT_SUMMARY / \"QA_dotplot_numeric_matrix_Level2final.csv\"\n",
    "df_num.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Shape:\", df_num.shape)\n",
    "print(df_num.head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e6835-f9cd-4130-9fd2-c9a18e51648a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfm_scRNA)",
   "language": "python",
   "name": "tfm_scrna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
