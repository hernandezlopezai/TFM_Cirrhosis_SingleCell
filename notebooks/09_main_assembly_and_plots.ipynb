{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71579c6a-18cb-4cb2-94e2-f8459feff7c7",
   "metadata": {},
   "source": [
    "**Nota importante**: La asignación de etiquetas Level2 se ha realizado combinando:\n",
    "- paneles de marcadores derivados del Inflammation PBMCs Atlas,\n",
    "- análisis de expresión diferencial 1-vs-rest por cluster L2,\n",
    "y\n",
    "- criterios biológicos consensuados con la tutora del TFM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e310284-a678-4e9d-9155-599e5592f1bc",
   "metadata": {},
   "source": [
    "# 09 – Ensamblado del objeto principal anotado y figuras globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718e3b2-cd5f-47f2-b0ac-baf21575b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Scanpy:\", sc.__version__)\n",
    "print(\"AnnData:\", ad.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9e188-ba92-4673-bbb4-0b810e538ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Celda 5 — Código: rutas del proyecto y configuración básica\n",
    "\n",
    "# Detectamos la carpeta raíz del proyecto asumiendo que este notebook está en \"notebooks/\"\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "AI_PACKAGE_DIR      = PROJECT_ROOT / \"AI_Package\"\n",
    "MARKERS_DIR         = AI_PACKAGE_DIR / \"markers\"\n",
    "DATA_PROCESSED_DIR  = PROJECT_ROOT / \"data_processed\"\n",
    "LINEAGES_DIR        = DATA_PROCESSED_DIR / \"lineages\"\n",
    "FIGURES_DIR         = PROJECT_ROOT / \"figures\"\n",
    "\n",
    "DATA_PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "LINEAGES_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Directorio del notebook:\", NOTEBOOK_DIR)\n",
    "print(\"Raíz del proyecto:\", PROJECT_ROOT)\n",
    "print(\"Carpeta AI_Package:\", AI_PACKAGE_DIR)\n",
    "print(\"Carpeta markers:\", MARKERS_DIR)\n",
    "print(\"Carpeta data_processed:\", DATA_PROCESSED_DIR)\n",
    "print(\"Carpeta lineages:\", LINEAGES_DIR)\n",
    "print(\"Carpeta figures:\", FIGURES_DIR)\n",
    "\n",
    "# Leemos MANIFEST.json para obtener claves estándar\n",
    "manifest_path = AI_PACKAGE_DIR / \"MANIFEST.json\"\n",
    "if manifest_path.exists():\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        manifest = json.load(f)\n",
    "else:\n",
    "    manifest = {}\n",
    "    print(\"\\n[AVISO] No se ha encontrado MANIFEST.json; se usarán valores por defecto.\")\n",
    "\n",
    "EMB_KEY = manifest.get(\"keys\", {}).get(\"EMB_KEY\", \"X_pca_harmony\")\n",
    "NBR_KEY = manifest.get(\"keys\", {}).get(\"NBR_KEY\", \"harmony\")\n",
    "UMAP_KEY = manifest.get(\"keys\", {}).get(\"UMAP_KEY\", \"X_umap_harmony\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"PROJECT_ROOT\": PROJECT_ROOT,\n",
    "    \"AI_PACKAGE_DIR\": AI_PACKAGE_DIR,\n",
    "    \"MARKERS_DIR\": MARKERS_DIR,\n",
    "    \"DATA_PROCESSED_DIR\": DATA_PROCESSED_DIR,\n",
    "    \"LINEAGES_DIR\": LINEAGES_DIR,\n",
    "    \"FIGURES_DIR\": FIGURES_DIR,\n",
    "    \"MAIN_WITH_L1_PATH\": DATA_PROCESSED_DIR / \"TFM_CIRRHOSIS_main_with_Level1.h5ad\",\n",
    "    \"EMB_KEY\": EMB_KEY,\n",
    "    \"NBR_KEY\": NBR_KEY,\n",
    "    \"UMAP_KEY\": UMAP_KEY,\n",
    "}\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91140060-3dd5-413c-97b5-c76823f43b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_with_l1_path = CONFIG[\"MAIN_WITH_L1_PATH\"]\n",
    "\n",
    "if not main_with_l1_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encuentra el objeto global con Level1 en:\\n{main_with_l1_path}\\n\"\n",
    "        \"Asegúrate de haber ejecutado el notebook de Level1 o ajusta la ruta.\"\n",
    "    )\n",
    "\n",
    "adata_main = sc.read_h5ad(main_with_l1_path)\n",
    "print(adata_main)\n",
    "\n",
    "# Comprobamos Level1\n",
    "if \"Level1\" not in adata_main.obs.columns:\n",
    "    raise KeyError(\"No se encontró 'Level1' en adata_main.obs.\")\n",
    "\n",
    "print(\"\\nDistribución de Level1 en el objeto global:\")\n",
    "print(adata_main.obs[\"Level1\"].value_counts())\n",
    "\n",
    "# Comprobamos que están los embeddings estándar\n",
    "for key in [CONFIG[\"EMB_KEY\"], CONFIG[\"UMAP_KEY\"]]:\n",
    "    if key not in adata_main.obsm.keys():\n",
    "        raise KeyError(f\"No se encontró '{key}' en adata_main.obsm.\")\n",
    "    else:\n",
    "        print(f\"Embedding '{key}' presente con forma {adata_main.obsm[key].shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f025be-598c-4285-8700-988e2250d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_categories = list(adata_main.obs[\"Level1\"].cat.categories) \\\n",
    "    if hasattr(adata_main.obs[\"Level1\"], \"cat\") \\\n",
    "    else sorted(adata_main.obs[\"Level1\"].unique())\n",
    "\n",
    "print(\"Linajes Level1 presentes en el objeto global:\")\n",
    "print(level1_categories)\n",
    "\n",
    "# Solo estos linajes tienen Level2 detallado (NB07–08 ejecutados):\n",
    "#   - B\n",
    "#   - T_and_NK\n",
    "#   - Mono_and_DC\n",
    "LINEAGES_WITH_LEVEL2 = [\"B\", \"T_and_NK\", \"Mono_and_DC\"]\n",
    "\n",
    "# Construimos las rutas esperadas de los archivos de linaje con L2 markers\n",
    "lineage_paths = {}\n",
    "print(\"\\nRutas esperadas de objetos por linaje con L2 markers (solo linajes con Level2):\")\n",
    "for l1 in level1_categories:\n",
    "    if l1 not in LINEAGES_WITH_LEVEL2:\n",
    "        print(f\"- {l1}: sin Level2 (se tratará como población única de Level1).\")\n",
    "        continue\n",
    "\n",
    "    safe_l1 = l1.replace(\" \", \"_\")\n",
    "    # Usamos el archivo generado en el notebook 08\n",
    "    path_l2 = LINEAGES_DIR / f\"TFM_CIRRHOSIS_Level1_{safe_l1}_L2markers.h5ad\"\n",
    "    lineage_paths[l1] = path_l2\n",
    "    print(f\"- {l1}: {path_l2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c4617-27f2-4d1a-bb3f-715523000c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL2_MAP solo se usa ahora para los linajes grandes con Level2 real:\n",
    "#   - T_and_NK\n",
    "#   - Mono_and_DC\n",
    "#   - B\n",
    "# pDC, Plasma, RBC, HSCs se tratan como poblaciones únicas a nivel de Level1.\n",
    "\n",
    "LEVEL2_MAP = {\n",
    "    \"T_and_NK\": {\n",
    "        \"0\":  \"NK\",                      # GZMB/NKG7/PRF1/TYROBP\n",
    "        \"1\":  \"CD8_Effector_Cytotoxic\",  # CCL5/GZMH/NKG7 (cytotoxic)\n",
    "        \"2\":  \"Conv_T_other\",            # IL7R/S100A4/LTB = T conv memoria (equivalente legacy)\n",
    "        \"3\":  \"CD4_Naive\",               # LEF1+ naive\n",
    "        \"4\":  \"CD4_Naive\",               # estado naive-like (validar CCR7/TCF7/LEF1)\n",
    "        \"5\":  \"CD4_Naive\",               # ribosomal-high; mantener pero marcar QC\n",
    "        \"6\":  \"CD8_Naive\",               # CD8B+ naive\n",
    "        \"7\":  \"Conv_T_other\",            # mito-high sin identidad clara; fallback legacy + QC flag\n",
    "        \"8\":  \"NK\",                      # mito-high + ZEB2 (effector/NK-like); QC flag\n",
    "        \"9\":  \"Treg\",                    # IKZF2/RTKN2\n",
    "        \"10\": \"MAIT\",                    # SLC4A10/KLRB1\n",
    "        \"11\": \"NK\",                      # GNLY/NCAM1\n",
    "        \"12\": \"T_NK_doublets\",           # GNLY + AOAH + CD247 (mixto; posible myeloid signal)\n",
    "        \"13\": \"Platelet_like_T\",         # PPBP/PF4\n",
    "        \"14\": \"NK\",                      # GNLY + ABCB1 (NK-like). No TRGC/TRDC => no gamma-delta\n",
    "        \"15\": \"CD8_Effector_Cytotoxic\",  # GZMK/LYST (effector-memory cytotoxic; sin etiqueta nueva)\n",
    "        \"16\": \"Myeloid_like_T\",          # S100A8/A9/DEFA3/LTF\n",
    "    },\n",
    "\n",
    "    \"Mono_and_DC\": {\n",
    "        \"0\":  \"Classical_Mono\",      # S100A8/S100A9/S100A12, VCAN\n",
    "        \"1\":  \"Classical_Mono\",      # AOAH/DPYD (mono state)\n",
    "        \"2\":  \"Classical_Mono\",      # IEG/activación (FOS, NFKBIA) pero sigue siendo mono\n",
    "        \"3\":  \"NonClassical_Mono\",   # FCGR3A, LST1, MS4A7\n",
    "        \"4\":  \"Classical_Mono\",      # S100A8 + FOS/JUN (activado)\n",
    "        \"5\":  \"Classical_Mono\",      # ribosomal-high dentro del linaje (validar QC, pero no inventar subtipo)\n",
    "        \"6\":  \"NonClassical_Mono\",   # HLA-II + FCGR3A (NC/mono MHC-II)\n",
    "        \"7\":  \"MonoDC_Other\",        # T-like doublets (CD247/PRKCH/STAT4)\n",
    "        \"8\":  \"ISG_Myeloid\",         # IFI44/EPSTI1/MX1/ISG15 (legacy tenía esta clase)\n",
    "        \"9\":  \"MonoDC_Other\",        # ENG/TIMP1 → stroma/endo-like doublet/contaminación\n",
    "        \"10\": \"cDC2\",                # CD74 + HLA-DQ/DP/DR (y debería salir CD1C/FCER1A en dotplot)\n",
    "        \"11\": \"DC4\",                 # TCF7L2/LYN/MTSS1: aceptar DC4 provisional; validar ITGAX/FCGR3A/LILRB2/SIGLEC10\n",
    "        \"12\": \"DC3\",                 # provisional: validar CD1C + S100A8/9 + ANXA1 (si no, degradar a MonoDC_Other intermedio)\n",
    "        \"13\": \"MonoDC_Other\",        # B-like doublets (MS4A1/BANK1/BACH2)\n",
    "        \"14\": \"cDC1\",                # CLEC9A (muy claro)\n",
    "        \"15\": \"MonoDC_Other\",        # progenitor-like (RUNX1/HDC/MSI2) como legacy\n",
    "    },\n",
    "\n",
    "    \"B\": {\n",
    "        \"0\":  \"B_Naive\",      # Canonical (BACH2, TCL1A)\n",
    "        \"1\":  \"B_Naive\",      # (ver nota abajo: revisar porque top markers salen ribosomales); Ribosomal (RPL/RPS high)\n",
    "        \"2\":  \"B_Memory\",     # Canonical (BANK1, TBC1D9)\n",
    "        \"3\":  \"B_Memory\",     # AIM2+ (Inflammatory/Memory)\n",
    "        \"4\":  \"B_Atypical\",   # FCRL5+, SOX5+, ABC-like\n",
    "        \"5\":  \"B_Other\",      # T-like (PRKCH, CD247, BCL11B, FYB1...); T-cell Doublets (CD247, BCL11B) -> Cleanup target\n",
    "        \"6\":  \"B_Memory\",     # (incierto; ver validaciones); Ribosomal small cluster, leans Memory\n",
    "        \"7\":  \"B_Naive\",      # BACH2/AFF3/FOXP1 sugiere B “naive/early”; Stressed/Mito high, markers match Naive\n",
    "        \"8\":  \"B_Activated\",  # (incierto; ver validaciones); FNBP1+\n",
    "        \"9\":  \"B_Activated\",  # (incierto; ver validaciones); ABCB1+\n",
    "        \"10\": \"B_Activated\",  # SAMSN1, CD82, etc. cuadra con activación; SAMSN1+, CD82+\n",
    "        \"11\": \"B_Other\",      # Platelet-like (PPBP, PF4, NRGN...); Platelet Doublets (PPBP, PF4) -> Cleanup target\n",
    "        \"12\": \"B_Other\",      # Myeloid-like (FCN1...); Myeloid Doublets (FCN1) -> Cleanup target\n",
    "        \"13\": \"B_Other\",      # probable contaminante (S100A8...); Myeloid/Junk Doublets (S100A8)\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"LEVEL2_MAP definido para los siguientes linajes (con Level2 real):\")\n",
    "for l1 in LEVEL2_MAP.keys():\n",
    "    print(f\" - {l1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1df57-05fe-4020-ae2d-0eded15c3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Integración de Level2 desde objetos por linaje (aplicando LEVEL2_MAP cuando está definido)\n",
    "\n",
    "# Diccionario global: cell_id -> Level2_label (biológica si existe mapeo)\n",
    "cell_to_level2 = {}\n",
    "\n",
    "for l1, path in lineage_paths.items():\n",
    "    if not path.exists():\n",
    "        print(f\"[AVISO] No se ha encontrado el archivo por linaje para '{l1}' en:\\n  {path}\\n\"\n",
    "              \"Este linaje no aportará anotación Level2.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcesando linaje '{l1}' desde archivo:\\n  {path}\")\n",
    "    adata_l1 = sc.read_h5ad(path)\n",
    "    print(adata_l1)\n",
    "\n",
    "    # Comprobaciones básicas\n",
    "    if \"Level1\" not in adata_l1.obs.columns:\n",
    "        raise KeyError(f\"No se encontró 'Level1' en el objeto por linaje '{l1}'.\")\n",
    "    unique_l1 = adata_l1.obs[\"Level1\"].unique()\n",
    "    print(\"  Valores únicos de Level1 en el objeto de linaje:\", unique_l1)\n",
    "    if len(unique_l1) != 1 or unique_l1[0] != l1:\n",
    "        raise ValueError(\n",
    "            f\"  El objeto por linaje para '{l1}' contiene Level1 = {unique_l1}, \"\n",
    "            f\"pero se esperaba solo '{l1}'.\"\n",
    "        )\n",
    "\n",
    "    # Decidimos qué columna usar como fuente de clusters L2 dentro de este objeto\n",
    "    if \"leiden_L2\" in adata_l1.obs.columns:\n",
    "        base_l2_col = \"leiden_L2\"\n",
    "        print(\"  Usando 'leiden_L2' como columna base de clusters L2.\")\n",
    "    elif \"Level2\" in adata_l1.obs.columns:\n",
    "        base_l2_col = \"Level2\"\n",
    "        print(\"  No se encontró 'leiden_L2'; usando 'Level2' como columna base de clusters L2.\")\n",
    "    else:\n",
    "        print(f\"  [AVISO] No hay 'leiden_L2' ni 'Level2' en el objeto de linaje '{l1}'. \"\n",
    "              \"No se podrá integrar Level2 para este linaje.\")\n",
    "        continue\n",
    "\n",
    "    # Convertimos la columna base a string (claves del diccionario LEVEL2_MAP)\n",
    "    l2_raw = adata_l1.obs[base_l2_col].astype(str)\n",
    "\n",
    "    # Aplicamos mapeo biológico si está definido para este linaje\n",
    "    mapping = LEVEL2_MAP.get(l1, {})\n",
    "    if mapping:\n",
    "        print(\"  Aplicando LEVEL2_MAP para este linaje.\")\n",
    "        l2_bio = l2_raw.map(mapping)\n",
    "\n",
    "        # Donde el mapeo no esté definido, mantenemos el identificador de cluster original\n",
    "        l2_bio = l2_bio.fillna(l2_raw)\n",
    "    else:\n",
    "        print(\"  [INFO] No hay LEVEL2_MAP definido para este linaje; \"\n",
    "              \"se usan directamente los identificadores de cluster L2.\")\n",
    "        l2_bio = l2_raw\n",
    "\n",
    "    # Guardamos también en el objeto de linaje (por si quieres reutilizarlo más adelante)\n",
    "    adata_l1.obs[\"Level2_bio\"] = l2_bio.astype(\"category\")\n",
    "\n",
    "    # Extraemos las etiquetas de Level2 biológicas y las mapeamos por obs_names\n",
    "    for cell_id, l2_label in zip(adata_l1.obs_names, adata_l1.obs[\"Level2_bio\"]):\n",
    "        if cell_id in cell_to_level2:\n",
    "            # En principio no debería ocurrir; lo avisamos si pasa\n",
    "            print(f\"  [AVISO] Célula {cell_id} ya tenía Level2 asignado. Sobrescribiendo.\")\n",
    "        cell_to_level2[cell_id] = l2_label\n",
    "\n",
    "# Ahora creamos una serie alineada con adata_main.obs_names\n",
    "level2_series = pd.Series(index=adata_main.obs_names, dtype=\"object\")\n",
    "\n",
    "n_matched = 0\n",
    "for cell_id in adata_main.obs_names:\n",
    "    if cell_id in cell_to_level2:\n",
    "        level2_series.loc[cell_id] = cell_to_level2[cell_id]\n",
    "        n_matched += 1\n",
    "\n",
    "print(f\"\\nCélulas del objeto global con Level2 asignado: {n_matched} de {adata_main.n_obs}.\")\n",
    "\n",
    "# Asignamos la columna Level2 (puede haber NaN si algún linaje no aportó Level2)\n",
    "adata_main.obs[\"Level2\"] = level2_series.astype(\"category\")\n",
    "print(\"\\nResumen de Level2 (incluyendo posibles NaN):\")\n",
    "print(adata_main.obs[\"Level2\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e9afb-491a-4b6c-996f-efac832fe1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de una etiqueta combinada Type_L1L2 (anotación \"fina\" en una sola columna)\n",
    "\n",
    "if \"Level1\" not in adata_main.obs.columns:\n",
    "    raise KeyError(\"No se encontró 'Level1' en adata_main.obs.\")\n",
    "\n",
    "# Siempre partimos de Level1 como etiqueta base\n",
    "l1_str = adata_main.obs[\"Level1\"].astype(str)\n",
    "\n",
    "if \"Level2\" in adata_main.obs.columns:\n",
    "    # Usamos Level1__Level2 solo en las células que tienen Level2 definido (no NaN)\n",
    "    l2 = adata_main.obs[\"Level2\"]\n",
    "    has_l2 = l2.notna()\n",
    "    l2_str = l2.astype(str)\n",
    "\n",
    "    # Por defecto: sólo Level1; donde haya Level2, concatenamos\n",
    "    type_l1l2 = l1_str.copy()\n",
    "    type_l1l2[has_l2] = l1_str[has_l2] + \"__\" + l2_str[has_l2]\n",
    "\n",
    "    type_l1l2.name = \"Type_L1L2\"\n",
    "    adata_main.obs[\"Type_L1L2\"] = type_l1l2.astype(\"category\")\n",
    "    print(\"\\nColumna 'Type_L1L2' creada combinando Level1 y Level2 donde está disponible.\")\n",
    "else:\n",
    "    # Fallback: si no hay Level2 como columna, usamos sólo Level1 como etiqueta fina\n",
    "    adata_main.obs[\"Type_L1L2\"] = adata_main.obs[\"Level1\"].astype(\"category\")\n",
    "    print(\"\\n[INFO] No se encontró 'Level2'; 'Type_L1L2' se ha definido igual que 'Level1'.\")\n",
    "\n",
    "print(\"Resumen de 'Type_L1L2':\")\n",
    "print(adata_main.obs[\"Type_L1L2\"].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573d6d6-cbf8-4e70-8739-19ec773dd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos lista de variables para colorear el UMAP\n",
    "color_vars = [\"Level1\"]\n",
    "if \"Level2\" in adata_main.obs.columns:\n",
    "    color_vars.append(\"Level2\")\n",
    "\n",
    "# Añadimos variables de interés si existen\n",
    "for col in [\"sample\", \"sample_id\", \"libraryID\", \"disease\", \"condition\", \"group\"]:\n",
    "    if col in adata_main.obs.columns:\n",
    "        color_vars.append(col)\n",
    "\n",
    "print(\"Variables que se usarán para colorear UMAP:\", color_vars)\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata_main,\n",
    "    basis=CONFIG[\"UMAP_KEY\"],\n",
    "    color=color_vars,\n",
    "    ncols=2,\n",
    "    frameon=False,\n",
    "    save=f\"_{'__'.join(color_vars[:4])}.png\"  # guarda una versión básica\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d388c-56ae-48b2-a701-0a8a93868496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if str(MARKERS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(MARKERS_DIR))\n",
    "\n",
    "try:\n",
    "    import markers\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"No se pudo importar 'markers.py' desde {MARKERS_DIR}.\\n\"\n",
    "        f\"Error original: {e}\"\n",
    "    )\n",
    "\n",
    "geneMarkers_level1 = getattr(markers, \"geneMarkers_level1\", {})\n",
    "geneMarkers_level2 = getattr(markers, \"geneMarkers_level2\", {})\n",
    "\n",
    "print(\"Level1 definidos en geneMarkers_level1:\")\n",
    "for l1 in geneMarkers_level1.keys():\n",
    "    print(f\"- {l1}\")\n",
    "\n",
    "print(\"\\nLevel1 disponibles en geneMarkers_level2:\")\n",
    "for l1 in geneMarkers_level2.keys():\n",
    "    print(f\"- {l1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721d4ed-2919-47d8-a856-90065d666ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if geneMarkers_level1:\n",
    "    # Construimos diccionario de var_names realmente presentes, igual que en el notebook 06\n",
    "    varnames_level1 = {}\n",
    "    for l1, genes in geneMarkers_level1.items():\n",
    "        varnames = markers.symbols_to_varnames(adata_main, genes)\n",
    "        if varnames:\n",
    "            varnames_level1[l1] = varnames\n",
    "\n",
    "    if varnames_level1:\n",
    "        print(\"Generando dotplot global de marcadores Level1 (usando solo genes presentes)...\")\n",
    "        sc.pl.dotplot(\n",
    "            adata_main,\n",
    "            var_names=varnames_level1,\n",
    "            groupby=\"Level1\",\n",
    "            standard_scale=\"var\",\n",
    "            dendrogram=True,\n",
    "            save=\"_Level1_dotplot.png\",\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"[AVISO] Ninguno de los genes de geneMarkers_level1 se ha podido mapear a var_names; \"\n",
    "            \"se omite el dotplot global de Level1.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"[AVISO] geneMarkers_level1 está vacío o no definido; se omite el dotplot global de Level1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d96da4-b150-4f1a-938e-74f31d25f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1 in level1_categories:\n",
    "    # Solo hacemos dotplot L2 para los linajes con Level2 real\n",
    "    if l1 not in LINEAGES_WITH_LEVEL2:\n",
    "        print(f\"[INFO] Se omite dotplot L2 para '{l1}' (sin Level2; población única).\")\n",
    "        continue\n",
    "\n",
    "    if l1 not in geneMarkers_level2:\n",
    "        print(f\"[INFO] No hay paneles Level2 definidos para '{l1}' en geneMarkers_level2. Se omite dotplot L2 para este linaje.\")\n",
    "        continue\n",
    "\n",
    "    # Subconjunto de células de este linaje\n",
    "    mask = (adata_main.obs[\"Level1\"] == l1).values\n",
    "    adata_l1 = adata_main[mask].copy()\n",
    "\n",
    "    if \"Level2\" not in adata_l1.obs.columns:\n",
    "        print(f\"[AVISO] El linaje '{l1}' no tiene 'Level2' definido en el objeto global. Se omite dotplot L2.\")\n",
    "        continue\n",
    "\n",
    "    # Panel de genes de Level2 para este linaje (dict: Level2 -> [genes])\n",
    "    panel_genes = geneMarkers_level2[l1]\n",
    "\n",
    "    # Mapeamos cada lista de genes a var_names reales del subobjeto\n",
    "    varnames_level2 = {}\n",
    "    for l2_name, genes in panel_genes.items():\n",
    "        varnames = markers.symbols_to_varnames(adata_l1, genes)\n",
    "        if varnames:\n",
    "            varnames_level2[l2_name] = varnames\n",
    "\n",
    "    if not varnames_level2:\n",
    "        print(\n",
    "            f\"[AVISO] Ningún gen del panel Level2 para '{l1}' se ha podido mapear a var_names; \"\n",
    "            \"se omite el dotplot L2 para este linaje.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(f\"Generando dotplot de Level2 para linaje '{l1}'...\")\n",
    "    # Aseguramos que Level2 es categórico\n",
    "    adata_l1.obs[\"Level2\"] = adata_l1.obs[\"Level2\"].astype(\"category\")\n",
    "\n",
    "    sc.pl.dotplot(\n",
    "        adata_l1,\n",
    "        var_names=varnames_level2,\n",
    "        groupby=\"Level2\",\n",
    "        standard_scale=\"var\",\n",
    "        dendrogram=True,\n",
    "        save=f\"_Level2_{l1.replace(' ', '_')}_dotplot.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cdc93-0df8-42d8-bd74-1372e7639697",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_DIR = DATA_PROCESSED_DIR / \"summary_tables\"\n",
    "SUMMARY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Tabla Level1 x Level2\n",
    "if \"Level2\" in adata_main.obs.columns:\n",
    "    ctab_L1_L2 = pd.crosstab(adata_main.obs[\"Level1\"], adata_main.obs[\"Level2\"])\n",
    "    path_L1_L2 = SUMMARY_DIR / \"cell_counts_Level1_by_Level2.csv\"\n",
    "    ctab_L1_L2.to_csv(path_L1_L2)\n",
    "    print(\"Guardada tabla de conteos Level1 x Level2 en:\")\n",
    "    print(\" \", path_L1_L2)\n",
    "else:\n",
    "    print(\"[INFO] 'Level2' no está definido globalmente; se omite la tabla Level1 x Level2.\")\n",
    "\n",
    "# Tabla Level1 x sample (si existe alguna columna tipo sample)\n",
    "sample_col = None\n",
    "for cand in [\"sample\", \"sample_id\", \"libraryID\"]:\n",
    "    if cand in adata_main.obs.columns:\n",
    "        sample_col = cand\n",
    "        break\n",
    "\n",
    "if sample_col is not None:\n",
    "    ctab_L1_sample = pd.crosstab(adata_main.obs[\"Level1\"], adata_main.obs[sample_col])\n",
    "    path_L1_sample = SUMMARY_DIR / f\"cell_counts_Level1_by_{sample_col}.csv\"\n",
    "    ctab_L1_sample.to_csv(path_L1_sample)\n",
    "    print(f\"Guardada tabla de conteos Level1 x {sample_col} en:\")\n",
    "    print(\" \", path_L1_sample)\n",
    "else:\n",
    "    print(\"[INFO] No se encontró columna de sample ('sample', 'sample_id' o 'libraryID'); se omite esta tabla.\")\n",
    "\n",
    "# Tabla Level1 x disease/condition (si existe)\n",
    "disease_col = None\n",
    "for cand in [\"disease\", \"condition\", \"group\"]:\n",
    "    if cand in adata_main.obs.columns:\n",
    "        disease_col = cand\n",
    "        break\n",
    "\n",
    "if disease_col is not None:\n",
    "    ctab_L1_dis = pd.crosstab(adata_main.obs[\"Level1\"], adata_main.obs[disease_col])\n",
    "    path_L1_dis = SUMMARY_DIR / f\"cell_counts_Level1_by_{disease_col}.csv\"\n",
    "    ctab_L1_dis.to_csv(path_L1_dis)\n",
    "    print(f\"Guardada tabla de conteos Level1 x {disease_col} en:\")\n",
    "    print(\" \", path_L1_dis)\n",
    "else:\n",
    "    print(\"[INFO] No se encontró columna de disease/condition ('disease', 'condition', 'group'); se omite esta tabla.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09d9ff-cf2d-46e3-ba96-fb1e9b55ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_annotated_path = DATA_PROCESSED_DIR / \"TFM_CIRRHOSIS_main_annotated.h5ad\"\n",
    "adata_main.write_h5ad(main_annotated_path)\n",
    "\n",
    "print(\"Objeto principal anotado guardado en:\")\n",
    "print(main_annotated_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd247d31-b157-4ebd-844c-6ed778256d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_main.obs[\"Level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902fa49-7023-4b32-8417-9874d74886c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_main.obs[\"Level2\"].value_counts(dropna=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df9d50-3d2c-4fdb-a33a-8a28a15909de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QA FINAL — Consistencia Level2 tras aplicar LEVEL2_MAP\n",
    "# (No modifica el objeto; solo imprime checks y posibles alertas)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QA FINAL — Level2 / Type_L1L2 (post-LEVEL2_MAP)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 0) Resumen básico\n",
    "print(\"\\n[0] Resumen básico\")\n",
    "print(\"  n_obs global:\", adata_main.n_obs)\n",
    "print(\"  Level1 únicos:\", adata_main.obs[\"Level1\"].nunique())\n",
    "print(\"  Level2 únicos (incluyendo NaN):\", adata_main.obs[\"Level2\"].nunique(dropna=False))\n",
    "print(\"  Type_L1L2 únicos:\", adata_main.obs[\"Type_L1L2\"].nunique())\n",
    "\n",
    "# 1) Cobertura: cuántas células tienen Level2\n",
    "print(\"\\n[1] Cobertura de Level2\")\n",
    "n_l2 = adata_main.obs[\"Level2\"].notna().sum()\n",
    "print(f\"  Células con Level2 asignado: {n_l2} / {adata_main.n_obs} ({100*n_l2/adata_main.n_obs:.2f}%)\")\n",
    "print(\"  NaN en Level2:\", adata_main.obs[\"Level2\"].isna().sum())\n",
    "\n",
    "# 2) ¿Se han colado clusters numéricos por falta de mapping?\n",
    "print(\"\\n[2] Labels numéricas en Level2 (indicador de mapping incompleto)\")\n",
    "l2_str = adata_main.obs[\"Level2\"].astype(str)\n",
    "is_numeric = l2_str.str.fullmatch(r\"\\d+\").fillna(False)\n",
    "n_numeric = int(is_numeric.sum())\n",
    "print(\"  Nº celdas con Level2 numérico:\", n_numeric)\n",
    "if n_numeric > 0:\n",
    "    print(\"  [ALERTA] Hay Level2 numéricos. Ejemplos:\")\n",
    "    print(adata_main.obs.loc[is_numeric, [\"Level1\",\"Level2\"]].head(20))\n",
    "\n",
    "# 3) Crosstab Level1 x Level2 (top por linaje)\n",
    "print(\"\\n[3] Crosstab Level1 x Level2 (top 10 por Level1)\")\n",
    "ct = pd.crosstab(adata_main.obs[\"Level1\"], adata_main.obs[\"Level2\"], dropna=False)\n",
    "\n",
    "for l1 in LINEAGES_WITH_LEVEL2:\n",
    "    if l1 in ct.index:\n",
    "        top = ct.loc[l1].sort_values(ascending=False).head(15)\n",
    "        print(f\"\\n  -- {l1} (top) --\")\n",
    "        print(top[top > 0])\n",
    "\n",
    "# 4) Universe check: categorías esperadas vs observadas (solo linajes con Level2 real)\n",
    "print(\"\\n[4] Universe check (solo linajes con Level2 real)\")\n",
    "for l1 in LINEAGES_WITH_LEVEL2:\n",
    "    obs_l2 = sorted(\n",
    "        adata_main.obs.loc[adata_main.obs[\"Level1\"] == l1, \"Level2\"]\n",
    "        .dropna().astype(str).unique().tolist()\n",
    "    )\n",
    "    map_l2 = sorted(set(LEVEL2_MAP.get(l1, {}).values()))\n",
    "    print(f\"\\n  Linaje: {l1}\")\n",
    "    print(\"   - Observadas en global:\", obs_l2)\n",
    "    print(\"   - En LEVEL2_MAP:\", map_l2)\n",
    "\n",
    "# 5) Tamaños de categorías \"sensibles\" (contaminación / doublets / Other)\n",
    "print(\"\\n[5] Tamaños de categorías sensibles (si existen)\")\n",
    "sensitive = [\n",
    "    \"Conv_T_other\", \"T_NK_doublets\", \"Platelet_like_T\", \"Myeloid_like_T\",\n",
    "    \"MonoDC_Other\", \"ISG_Myeloid\", \"B_Other\"\n",
    "]\n",
    "l2_vals = adata_main.obs[\"Level2\"].dropna().astype(str)\n",
    "for lab in sensitive:\n",
    "    if lab in set(l2_vals.unique()):\n",
    "        n = int((l2_vals == lab).sum())\n",
    "        print(f\"  {lab:>18}: {n}\")\n",
    "\n",
    "# 6) Si existe patientID: dominancia por paciente dentro de cada Level2 (top 10)\n",
    "print(\"\\n[6] Dominancia por patientID dentro de Level2 (si existe)\")\n",
    "if \"patientID\" in adata_main.obs.columns and adata_main.obs[\"Level2\"].notna().any():\n",
    "    tmp = pd.crosstab(adata_main.obs[\"Level2\"], adata_main.obs[\"patientID\"], normalize=\"index\").max(1)\n",
    "    print(tmp.sort_values(ascending=False).head(15))\n",
    "else:\n",
    "    print(\"  (patientID no disponible o Level2 vacío)\")\n",
    "\n",
    "# 7) Si existe scrublet: proporción de doublets por Level2 (top 15)\n",
    "print(\"\\n[7] Scrublet vs Level2 (si existe)\")\n",
    "if \"scrublet_predicted_doublet\" in adata_main.obs.columns and adata_main.obs[\"Level2\"].notna().any():\n",
    "    tab = pd.crosstab(adata_main.obs[\"Level2\"], adata_main.obs[\"scrublet_predicted_doublet\"], normalize=\"index\")\n",
    "    print(tab.head(20))\n",
    "else:\n",
    "    print(\"  (scrublet_predicted_doublet no disponible o Level2 vacío)\")\n",
    "\n",
    "print(\"\\nQA FINAL completado.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3be52-5e15-4114-9f3e-5258607a4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRA QA PARA DECIDIR SI NBXX ROMPERÁ ALGO (run al final de NB09)\n",
    "# - Solo lecturas/contajes: NO modifica el objeto\n",
    "# - Objetivo: ver si NBXX (listas T_L2/NK_L2/MONO_L2/DC_L2 + fills) encaja con tu Level2 actual\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- elegir objeto correcto ---\n",
    "if \"adata_main\" in globals():\n",
    "    A = adata_main\n",
    "elif \"adata\" in globals():\n",
    "    A = adata\n",
    "else:\n",
    "    raise NameError(\"No encuentro 'adata_main' ni 'adata' en memoria. Ejecuta antes la celda donde cargas/creas el objeto global.\")\n",
    "\n",
    "print(\"=== EXTRA QA post-NB09 ===\")\n",
    "print(\"n_obs:\", A.n_obs)\n",
    "\n",
    "# checks mínimos\n",
    "for col in [\"Level1\", \"Level2\"]:\n",
    "    if col not in A.obs.columns:\n",
    "        raise KeyError(f\"Falta columna '{col}' en A.obs\")\n",
    "\n",
    "# --- 1) NA en Level2 por Level1 (clave para el fill de NBXX) ---\n",
    "tmp = A.obs[[\"Level1\",\"Level2\"]].copy()\n",
    "tmp[\"Level2_isNA\"] = tmp[\"Level2\"].isna()\n",
    "na_by_l1 = tmp.groupby(\"Level1\")[\"Level2_isNA\"].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n[1] NA en Level2 por Level1 (top):\")\n",
    "print(na_by_l1.head(30))\n",
    "print(\"Total NA Level2:\", int(tmp[\"Level2_isNA\"].sum()))\n",
    "\n",
    "# --- 2) Universo real de Level2 (para comparar con listas hardcodeadas de NBXX) ---\n",
    "l2_present = sorted(pd.Series(A.obs[\"Level2\"]).dropna().astype(str).unique().tolist())\n",
    "print(\"\\n[2] Level2 presentes (sin NA) ->\", len(l2_present))\n",
    "print(l2_present)\n",
    "\n",
    "# --- 3) Crosstab Level1 x Level2 (para detectar incoherencias) ---\n",
    "ct = pd.crosstab(A.obs[\"Level1\"].astype(str), A.obs[\"Level2\"], dropna=False)\n",
    "print(\"\\n[3] Crosstab Level1 x Level2 (top por fila):\")\n",
    "for l1 in ct.index:\n",
    "    s = ct.loc[l1]\n",
    "    s = s[s > 0].sort_values(ascending=False)\n",
    "    print(f\"\\n-- {l1} --\")\n",
    "    print(s.head(40))\n",
    "\n",
    "# --- 4) Simulación NBXX: ¿qué Level2 quedarían SIN mapear a Level1_refined?\n",
    "T_L2 = [\n",
    "    \"CD4_Naive\",\"CD8_Naive\",\"CD8_Effector_Cytotoxic\",\"Conv_T_other\",\n",
    "    \"MAIT\",\"GammaDelta_T\",\"Treg\",\"Proliferative_T\",\"Exhausted_T\",\n",
    "]\n",
    "NK_L2 = [\"NK\"]\n",
    "MONO_L2 = [\"Classical_Mono\",\"NonClassical_Mono\",\"ISG_Myeloid\",\"MonoDC_Other\"]\n",
    "DC_L2   = [\"cDC1\",\"cDC2\",\"DC4\",\"aDC\"]\n",
    "\n",
    "obs = A.obs[[\"Level1\",\"Level2\"]].copy()\n",
    "obs[\"Level1\"] = obs[\"Level1\"].astype(str)\n",
    "mask_l2_na = obs[\"Level2\"].isna().to_numpy()\n",
    "obs[\"Level2\"] = obs[\"Level2\"].astype(str)\n",
    "\n",
    "mask_TNK = (obs[\"Level1\"].values == \"T_and_NK\") & (~mask_l2_na)\n",
    "mask_MDC = (obs[\"Level1\"].values == \"Mono_and_DC\") & (~mask_l2_na)\n",
    "\n",
    "mask_T    = mask_TNK & np.isin(obs[\"Level2\"].values, T_L2)\n",
    "mask_NK   = mask_TNK & np.isin(obs[\"Level2\"].values, NK_L2)\n",
    "mask_Mono = mask_MDC & np.isin(obs[\"Level2\"].values, MONO_L2)\n",
    "mask_DC   = mask_MDC & np.isin(obs[\"Level2\"].values, DC_L2)\n",
    "\n",
    "unmapped_TNK = int(mask_TNK.sum() - mask_T.sum() - mask_NK.sum())\n",
    "unmapped_MDC = int(mask_MDC.sum() - mask_Mono.sum() - mask_DC.sum())\n",
    "\n",
    "print(\"\\n[4] Simulación NBXX Level1_refined (estimación 'unmapped'):\")\n",
    "print(\"T_and_NK total (Level2 no-NA):\", int(mask_TNK.sum()))\n",
    "print(\" - mapearían a 'T' :\", int(mask_T.sum()))\n",
    "print(\" - mapearían a 'NK':\", int(mask_NK.sum()))\n",
    "print(\" - sin map (seguirían 'T_and_NK'):\", unmapped_TNK)\n",
    "\n",
    "print(\"\\nMono_and_DC total (Level2 no-NA):\", int(mask_MDC.sum()))\n",
    "print(\" - mapearían a 'Mono':\", int(mask_Mono.sum()))\n",
    "print(\" - mapearían a 'DC'  :\", int(mask_DC.sum()))\n",
    "print(\" - sin map (seguirían 'Mono_and_DC'):\", unmapped_MDC)\n",
    "\n",
    "# --- 5) Qué Level2 dentro de T_and_NK / Mono_and_DC NO están en las listas de NBXX ---\n",
    "l2_tnk = sorted(pd.Series(A.obs.loc[A.obs[\"Level1\"].astype(str)==\"T_and_NK\",\"Level2\"]).dropna().astype(str).unique())\n",
    "l2_mdc = sorted(pd.Series(A.obs.loc[A.obs[\"Level1\"].astype(str)==\"Mono_and_DC\",\"Level2\"]).dropna().astype(str).unique())\n",
    "\n",
    "unknown_in_TNK = sorted(set(l2_tnk) - set(T_L2) - set(NK_L2))\n",
    "unknown_in_MDC = sorted(set(l2_mdc) - set(MONO_L2) - set(DC_L2))\n",
    "\n",
    "print(\"\\n[5] Level2 en T_and_NK NO cubiertos por listas NBXX:\", unknown_in_TNK)\n",
    "print(\"[5] Level2 en Mono_and_DC NO cubiertos por listas NBXX:\", unknown_in_MDC)\n",
    "\n",
    "print(\"\\n[OK] EXTRA QA completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dab0e-aa86-42f9-83eb-62ba3705ba25",
   "metadata": {},
   "source": [
    "### Comentario de QC – Notebook 09 (post-Harmony, `TFM_CIRRHOSIS_main_annotated.h5ad`)\n",
    "\n",
    "Este comentario resume los controles de calidad realizados sobre el objeto global anotado en NB09 tras corregir el clustering por linaje (NB07) con Harmony. Sirve como **cierre técnico del NB09** y como recordatorio de puntos a vigilar para el downstream (NB10+).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Integración de Level2 y `Type_L1L2` (post-`LEVEL2_MAP`)\n",
    "\n",
    "- **Cobertura de Level2**:\n",
    "  - `Level2` asignado a **219,438 / 222,389** células (**98.67%**).\n",
    "  - Quedan **2,951 NaN** en `Level2`, consistente con que solo ciertos linajes tienen Level2 detallado (los demás quedan como “población única” a nivel Level1 y se tratan downstream).\n",
    "\n",
    "- **Calidad del mapeo**:\n",
    "  - **0 etiquetas numéricas** (no hay `\"0\"`, `\"1\"`, etc.) en `Level2` → indica que el `LEVEL2_MAP` cubre completamente los clusters L2 integrados.\n",
    "  - El “universo” de etiquetas observadas en `Level2` **coincide exactamente** con el universo definido en `LEVEL2_MAP` para los 3 linajes con Level2 real (`B`, `T_and_NK`, `Mono_and_DC`).\n",
    "\n",
    "- **`Type_L1L2`**:\n",
    "  - Se generan **28 categorías** de `Type_L1L2` (8 Level1, 24 Level2 incluyendo NaN), sin señales de incoherencia por categorías inesperadas.\n",
    "\n",
    "> **Conclusión:** La integración de Level2 desde los objetos por linaje y el uso del `LEVEL2_MAP` actualizado (post-Harmony) es técnicamente correcta: no hay pérdida masiva de células, no hay IDs numéricos “colados”, y el universo de etiquetas es consistente.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Coherencia Level1 × Level2 (crosstab)\n",
    "\n",
    "- **B** (Level2 observados):  \n",
    "  `B_Naive` 6,493; `B_Memory` 3,095; `B_Atypical` 1,125; `B_Activated` 598; `B_Other` 799.  \n",
    "  → Universo de B consistente y compacto.\n",
    "\n",
    "- **T_and_NK** (Level2 observados):  \n",
    "  `CD4_Naive` 36,685; `NK` 25,529; `Conv_T_other` 23,832; `CD8_Effector_Cytotoxic` 16,229; `CD8_Naive` 8,606; `Treg` 4,645; `MAIT` 1,576; `T_NK_doublets` 589; `Platelet_like_T` 583; `Myeloid_like_T` 152.  \n",
    "  → Se preservan explícitamente categorías de contaminación/doublets esperadas por downstream.\n",
    "\n",
    "- **Mono_and_DC** (Level2 observados):  \n",
    "  `Classical_Mono` 54,370; `NonClassical_Mono` 16,942; `MonoDC_Other` 8,651; `ISG_Myeloid` 4,246; `cDC2` 2,800; `DC4` 1,215; `DC3` 543; `cDC1` 135.  \n",
    "  → Universo de subtipos coherente con el mapa y con la estructura legacy (incluye `ISG_Myeloid` y `MonoDC_Other`).\n",
    "\n",
    "> **Conclusión:** No hay “subtipos fuera de su linaje”: las etiquetas Level2 aparecen en el Level1 esperado.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Categorías “sensibles” (doublet-like / contaminaciones / Other)\n",
    "\n",
    "Tamaños globales:\n",
    "- `Conv_T_other`: **23,832**\n",
    "- `T_NK_doublets`: **589**\n",
    "- `Platelet_like_T`: **583**\n",
    "- `Myeloid_like_T`: **152**\n",
    "- `MonoDC_Other`: **8,651**\n",
    "- `ISG_Myeloid`: **4,246**\n",
    "- `B_Other`: **799**\n",
    "\n",
    "Estas categorías eran parte del espíritu del pipeline legacy (clusters “no puros” que luego se gestionan en notebooks downstream). En el pipeline post-Harmony, siguen existiendo y con tamaños razonables.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Scrublet vs Level2 (validación indirecta de doublets)\n",
    "\n",
    "La distribución de `scrublet_predicted_doublet` es coherente con el uso downstream de categorías “doublet-like”:\n",
    "- `T_NK_doublets`: **~0.776 True** (muy enriquecido en doublets) → valida la etiqueta.\n",
    "- `B_Other`: **~0.457 True**, `MonoDC_Other`: **~0.353 True** → consistente con mezclas/contaminación.\n",
    "- `Myeloid_like_T`: **~0.072 True** (y N pequeño) → categoría rara, a vigilar pero no contradictoria.\n",
    "\n",
    "> **Conclusión:** Scrublet apoya que las clases “doublet/other” capturan artefactos reales que el legacy también trataba.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Variabilidad por paciente (banderas a vigilar, no “errores”)\n",
    "\n",
    "Se observa dominancia por `patientID` particularmente alta en:\n",
    "- `DC3`: **max ≈ 0.823** (posible población muy paciente-específica o artefacto residual)\n",
    "- `B_Activated`: **max ≈ 0.543**\n",
    "\n",
    "> **Implicación para NB10+:** Las comparaciones deben hacerse **por paciente** (proporciones por muestra) y no solo por pooling global, y conviene revisar que resultados de poblaciones raras no estén dominados por 1 individuo.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Diferencias importantes vs QC legacy (sin cambiar el diseño)\n",
    "\n",
    "- En legacy, `Level2` tenía 0 NaN porque se “completaba” el universo de poblaciones simples (Plasma/pDC/RBC/HSC/etc.).  \n",
    "  En post-Harmony, hay NaN en `Level2` para linajes sin Level2 detallado en NB07–08. Esto no invalida NB09, pero implica que **el rellenado final de Level2 para poblaciones simples (si se desea) debe ocurrir en notebooks downstream** (p.ej. limpieza final / Level2_final).\n",
    "\n",
    "- El universo de T/NK aquí no incluye explícitamente `GammaDelta_T`, `Proliferative_T` o `Exhausted_T` (presentes en legacy).  \n",
    "  Esto no es un “fallo”, pero **cualquier notebook downstream que asuma esas etiquetas debe revisarse** (o bien porque ya no existen como clusters separados, o porque se capturan dentro de buckets más amplios).\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Conclusión general del QC post-Harmony\n",
    "\n",
    "- La anotación **Level1 + Level2** del objeto global es **coherente** con el mapa actualizado y los checks técnicos (sin IDs numéricos, universo consistente, tamaños razonables).\n",
    "- Las categorías “problemáticas” (doublets/contaminación) siguen presentes y **están respaldadas por Scrublet**.\n",
    "- Hay señales de **heterogeneidad por paciente** en poblaciones raras (especialmente `DC3`), que deben considerarse explícitamente en NB10+.\n",
    "\n",
    "> **Estado final NB09 (post-Harmony):** Objeto global anotado listo para downstream, con la nota de que el “completado” de etiquetas para poblaciones simples y/o la normalización de universos legacy (si se requiere) se gestionará downstream (p.ej. Level2_final / limpieza final).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
