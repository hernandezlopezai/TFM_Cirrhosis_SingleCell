{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e834c03-f317-40be-aeb0-0021c99350f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QA Level2_final coherence + QA específica para HSCs\n",
    "# Salidas (summary_tables_final/):\n",
    "#  1) QA_Level2final_neighbor_purity.csv\n",
    "#  2) QA_HSCs_neighbor_label_distribution.csv\n",
    "#  3) QA_Level2final_umap_compactness.csv\n",
    "#  4) QA_HSCs_marker_panel_vs_rest.csv   (+ recomendación automática de 2 marcadores)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data_processed\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No encuentro 'data_processed' subiendo desde: {start}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\"\n",
    "OUT_SUM = PROJECT_ROOT / \"summary_tables_final\"\n",
    "OUT_SUM.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_FILTER = DATA_PROCESSED / \"TFM_CIRRHOSIS_main_filtered_for_analysis.h5ad\"\n",
    "MAP_PATH = OUT_SUM / \"Level2_final_map.json\"\n",
    "EMB_PATH = OUT_SUM / \"UMAP_Harmony_embeddings.csv\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"OUT_FILTER :\", OUT_FILTER)\n",
    "print(\"MAP_PATH   :\", MAP_PATH)\n",
    "print(\"EMB_PATH   :\", EMB_PATH)\n",
    "\n",
    "if not OUT_FILTER.exists():\n",
    "    raise FileNotFoundError(f\"No existe OUT_FILTER: {OUT_FILTER}\")\n",
    "if not MAP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"No existe Level2_final_map.json: {MAP_PATH}\")\n",
    "if not EMB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"No existe UMAP_Harmony_embeddings.csv: {EMB_PATH}\")\n",
    "\n",
    "with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    level2_map = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Coherencia Level2_final vía pureza de vecinos (grafo Harmony)\n",
    "# -----------------------------\n",
    "adata_b = sc.read_h5ad(OUT_FILTER, backed=\"r\")\n",
    "\n",
    "need = [\"patientID\", \"disease\", \"Level1_refined\", \"Level2\"]\n",
    "missing = [c for c in need if c not in adata_b.obs.columns]\n",
    "if missing:\n",
    "    adata_b.file.close()\n",
    "    raise KeyError(f\"Faltan columnas obs: {missing}\")\n",
    "\n",
    "obs = adata_b.obs[need].copy()\n",
    "\n",
    "# IMPORTANT: NO convertir Level2 a str antes de mapear (para no convertir NaN -> \"nan\")\n",
    "l2_obj = obs[\"Level2\"].astype(\"object\")\n",
    "obs[\"Level2_final\"] = l2_obj.replace(level2_map)\n",
    "obs[\"Level2_final\"] = obs[\"Level2_final\"].astype(\"object\")\n",
    "\n",
    "# Elegir conectividades: preferir harmony_connectivities si existe\n",
    "if \"harmony_connectivities\" in adata_b.obsp.keys():\n",
    "    C = adata_b.obsp[\"harmony_connectivities\"]\n",
    "    graph_name = \"harmony_connectivities\"\n",
    "elif \"connectivities\" in adata_b.obsp.keys():\n",
    "    C = adata_b.obsp[\"connectivities\"]\n",
    "    graph_name = \"connectivities\"\n",
    "else:\n",
    "    adata_b.file.close()\n",
    "    raise KeyError(\"No encuentro connectivities ni harmony_connectivities en obsp.\")\n",
    "\n",
    "print(f\"[OK] Usando grafo: obsp['{graph_name}'] -> {C.shape}\")\n",
    "\n",
    "# K: si está disponible en uns['neighbors'], usarlo; si no, fallback a 15\n",
    "K = 15\n",
    "try:\n",
    "    K = int(adata_b.uns.get(\"neighbors\", {}).get(\"params\", {}).get(\"n_neighbors\", 15))\n",
    "except Exception:\n",
    "    K = 15\n",
    "print(\"K (n_neighbors):\", K)\n",
    "\n",
    "def topk_neighbors_from_connectivities(C, k=15):\n",
    "    if not sp.issparse(C):\n",
    "        C = sp.csr_matrix(C)\n",
    "    C = C.tocsr()\n",
    "    n = C.shape[0]\n",
    "    neigh_idx = []\n",
    "\n",
    "    for i in range(n):\n",
    "        start, end = C.indptr[i], C.indptr[i + 1]\n",
    "        cols = C.indices[start:end]\n",
    "        data = C.data[start:end]\n",
    "\n",
    "        # quitar self\n",
    "        m = cols != i\n",
    "        cols = cols[m]\n",
    "        data = data[m]\n",
    "\n",
    "        if cols.size == 0:\n",
    "            neigh_idx.append(np.array([], dtype=int))\n",
    "            continue\n",
    "\n",
    "        # limitar a top-k por peso\n",
    "        if cols.size > k:\n",
    "            sel = np.argpartition(-data, k - 1)[:k]\n",
    "            cols = cols[sel]\n",
    "            data = data[sel]\n",
    "            ord2 = np.argsort(-data)\n",
    "            cols = cols[ord2]\n",
    "\n",
    "        neigh_idx.append(cols.astype(int))\n",
    "\n",
    "    return neigh_idx\n",
    "\n",
    "neigh_idx = topk_neighbors_from_connectivities(C, k=K)\n",
    "\n",
    "labels = obs[\"Level2_final\"].astype(str).to_numpy()\n",
    "all_levels = pd.Index(sorted(pd.unique(labels)))\n",
    "\n",
    "# per-cell same-label fraction\n",
    "same_frac = np.full(labels.shape[0], np.nan, dtype=float)\n",
    "for i in range(labels.shape[0]):\n",
    "    nb = neigh_idx[i]\n",
    "    if nb.size == 0:\n",
    "        continue\n",
    "    same_frac[i] = np.mean(labels[nb] == labels[i])\n",
    "\n",
    "df_purity = []\n",
    "for lvl in all_levels:\n",
    "    m = labels == lvl\n",
    "    v = same_frac[m]\n",
    "    df_purity.append({\n",
    "        \"Level2_final\": lvl,\n",
    "        \"n_cells\": int(m.sum()),\n",
    "        \"mean_sameLabel_inNeighbors\": float(np.nanmean(v)),\n",
    "        \"median_sameLabel_inNeighbors\": float(np.nanmedian(v)),\n",
    "        \"p25_sameLabel_inNeighbors\": float(np.nanpercentile(v, 25)),\n",
    "        \"p75_sameLabel_inNeighbors\": float(np.nanpercentile(v, 75)),\n",
    "        \"na_cells\": int(np.isnan(v).sum()),\n",
    "    })\n",
    "\n",
    "df_purity = pd.DataFrame(df_purity).sort_values(\n",
    "    [\"mean_sameLabel_inNeighbors\", \"n_cells\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "purity_path = OUT_SUM / \"QA_Level2final_neighbor_purity.csv\"\n",
    "df_purity.to_csv(purity_path, index=False)\n",
    "print(\"\\nSaved:\", purity_path)\n",
    "print(df_purity.head(10).to_string(index=False))\n",
    "\n",
    "# distribución de etiquetas vecinas para HSCs (¿con quién se “pega”?)\n",
    "hsc_mask = labels == \"HSCs\"\n",
    "if int(hsc_mask.sum()) > 0:\n",
    "    counts = {}\n",
    "    idx_hsc = np.where(hsc_mask)[0]\n",
    "    for i in idx_hsc:\n",
    "        for j in neigh_idx[i]:\n",
    "            lab = labels[j]\n",
    "            counts[lab] = counts.get(lab, 0) + 1\n",
    "\n",
    "    df_hsc_nb = (\n",
    "        pd.DataFrame({\"neighbor_label\": list(counts.keys()), \"n_edges\": list(counts.values())})\n",
    "          .sort_values(\"n_edges\", ascending=False)\n",
    "    )\n",
    "    df_hsc_nb[\"share\"] = df_hsc_nb[\"n_edges\"] / df_hsc_nb[\"n_edges\"].sum()\n",
    "\n",
    "    hsc_nb_path = OUT_SUM / \"QA_HSCs_neighbor_label_distribution.csv\"\n",
    "    df_hsc_nb.to_csv(hsc_nb_path, index=False)\n",
    "    print(\"\\nSaved:\", hsc_nb_path)\n",
    "    print(df_hsc_nb.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n[WARN] No hay HSCs en Level2_final (n=0). Se omite QA_HSCs_neighbor_label_distribution.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Compactación en UMAP (sin colores): radio medio a centroide\n",
    "# -----------------------------\n",
    "# Si por cualquier razón el CSV no tuviera Level2_final, lo reconstruimos\n",
    "emb_cols = pd.read_csv(EMB_PATH, nrows=1).columns.tolist()\n",
    "usecols = [\"UMAP1_harmony\", \"UMAP2_harmony\"]\n",
    "if \"Level2_final\" in emb_cols:\n",
    "    usecols = [\"Level2_final\"] + usecols\n",
    "elif \"Level2\" in emb_cols:\n",
    "    usecols = [\"Level2\"] + usecols\n",
    "else:\n",
    "    raise KeyError(\"UMAP_Harmony_embeddings.csv no contiene ni Level2_final ni Level2.\")\n",
    "\n",
    "emb = pd.read_csv(EMB_PATH, usecols=usecols)\n",
    "\n",
    "if \"Level2_final\" not in emb.columns:\n",
    "    emb[\"Level2_final\"] = emb[\"Level2\"].astype(\"object\").replace(level2_map).astype(str)\n",
    "\n",
    "def compactness(df):\n",
    "    x = df[[\"UMAP1_harmony\", \"UMAP2_harmony\"]].to_numpy(float)\n",
    "    c = x.mean(axis=0)\n",
    "    d = np.sqrt(((x - c) ** 2).sum(axis=1))\n",
    "    return pd.Series({\n",
    "        \"n_cells\": int(x.shape[0]),\n",
    "        \"mean_radius\": float(d.mean()),\n",
    "        \"median_radius\": float(np.median(d)),\n",
    "        \"p75_radius\": float(np.percentile(d, 75)),\n",
    "        \"p95_radius\": float(np.percentile(d, 95)),\n",
    "    })\n",
    "\n",
    "df_comp = emb.groupby(\"Level2_final\", sort=False).apply(compactness).reset_index()\n",
    "comp_path = OUT_SUM / \"QA_Level2final_umap_compactness.csv\"\n",
    "df_comp.to_csv(comp_path, index=False)\n",
    "print(\"\\nSaved:\", comp_path)\n",
    "print(df_comp.sort_values(\"mean_radius\", ascending=False).head(10).to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Panel numérico para HSCs: marcadores alternativos vs resto\n",
    "#    (y recomendación automática de 2 genes para sustituir CD34/KIT si salen mal)\n",
    "# -----------------------------\n",
    "if int(hsc_mask.sum()) == 0:\n",
    "    print(\"\\n[WARN] No hay HSCs -> se omite panel de marcadores HSCs.\")\n",
    "    adata_b.file.close()\n",
    "    raise SystemExit(0)\n",
    "\n",
    "CANDIDATES = [\n",
    "    \"CD34\",\"KIT\",\n",
    "    \"SOX4\",\"SPINK2\",\"AVP\",\n",
    "    \"GATA2\",\"LMO2\",\"TAL1\",\"MEIS1\",\"MPL\",\"HOPX\",\n",
    "    \"HMGB2\",\"TMSB10\",\"TYMP\"\n",
    "]\n",
    "\n",
    "def symbol_to_varname(adata, symbol):\n",
    "    if symbol in adata.var_names:\n",
    "        return symbol\n",
    "    if \"symbol\" in adata.var.columns:\n",
    "        sym = adata.var[\"symbol\"].astype(str).to_numpy()\n",
    "        hits = np.where(sym == symbol)[0]\n",
    "        if hits.size > 0:\n",
    "            return adata.var_names[hits[0]]\n",
    "    return None\n",
    "\n",
    "varnames = []\n",
    "sym_kept = []\n",
    "for s in CANDIDATES:\n",
    "    v = symbol_to_varname(adata_b, s)\n",
    "    if v is not None:\n",
    "        varnames.append(v)\n",
    "        sym_kept.append(s)\n",
    "\n",
    "print(\"\\nHSC panel: símbolos encontrados =\", len(sym_kept), \"/\", len(CANDIDATES))\n",
    "if len(sym_kept) == 0:\n",
    "    adata_b.file.close()\n",
    "    raise RuntimeError(\"No encontré ninguno de los genes candidatos en var_names/var['symbol'].\")\n",
    "\n",
    "adata_small = adata_b[:, varnames].to_memory()\n",
    "adata_b.file.close()\n",
    "\n",
    "# elegir matriz a usar\n",
    "X = adata_small.layers[\"log1p_10k\"] if \"log1p_10k\" in adata_small.layers.keys() else adata_small.X\n",
    "if not sp.issparse(X):\n",
    "    X = sp.csr_matrix(X)\n",
    "X = X.tocsr()\n",
    "\n",
    "mask_hsc = (labels == \"HSCs\")\n",
    "mask_rest = ~mask_hsc\n",
    "\n",
    "def mean_and_frac(Xsub):\n",
    "    mean = np.asarray(Xsub.mean(axis=0)).ravel()\n",
    "    frac = np.asarray((Xsub > 0).mean(axis=0)).ravel()\n",
    "    return mean, frac\n",
    "\n",
    "mean_h, frac_h = mean_and_frac(X[mask_hsc])\n",
    "mean_r, frac_r = mean_and_frac(X[mask_rest])\n",
    "\n",
    "df_hsc_panel = pd.DataFrame({\n",
    "    \"marker\": sym_kept,\n",
    "    \"mean_log1p_HSCs\": mean_h,\n",
    "    \"frac_nonzero_HSCs\": frac_h,\n",
    "    \"mean_log1p_rest\": mean_r,\n",
    "    \"frac_nonzero_rest\": frac_r,\n",
    "})\n",
    "df_hsc_panel[\"delta_mean\"] = df_hsc_panel[\"mean_log1p_HSCs\"] - df_hsc_panel[\"mean_log1p_rest\"]\n",
    "df_hsc_panel[\"delta_frac\"] = df_hsc_panel[\"frac_nonzero_HSCs\"] - df_hsc_panel[\"frac_nonzero_rest\"]\n",
    "\n",
    "panel_path = OUT_SUM / \"QA_HSCs_marker_panel_vs_rest.csv\"\n",
    "df_hsc_panel.sort_values([\"mean_log1p_HSCs\", \"frac_nonzero_HSCs\"], ascending=False).to_csv(panel_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\", panel_path)\n",
    "print(df_hsc_panel.sort_values([\"mean_log1p_HSCs\",\"frac_nonzero_HSCs\"], ascending=False).head(10).to_string(index=False))\n",
    "\n",
    "# recomendación automática: top2 por \"señal\" dentro de HSCs (media * frac)\n",
    "df_hsc_panel[\"score\"] = df_hsc_panel[\"mean_log1p_HSCs\"] * df_hsc_panel[\"frac_nonzero_HSCs\"]\n",
    "top2 = df_hsc_panel.sort_values(\"score\", ascending=False).head(2)[\"marker\"].tolist()\n",
    "print(\"\\n[RECOMENDACIÓN] Marcadores HSCs (top2 por score=mean*frac):\", top2)\n",
    "\n",
    "print(\"\\n[OK] QA coherence + HSCs listo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfm_scRNA)",
   "language": "python",
   "name": "tfm_scrna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
